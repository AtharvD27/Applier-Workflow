name: Scrape & Filter Stealth

on:  
  schedule:
    - cron: '30 10 * * *'  # 6:30 AM EST
    - cron: '30 12 * * *'  # 8:30 AM EST
    - cron: '30 14 * * *'  # 10:30 AM EST
    - cron: '30 16 * * *'  # 12:30 PM EST
    - cron: '30 18 * * *'  # 2:30 PM EST
    - cron: '30 20 * * *'  # 4:30 PM EST
    - cron: '30 22 * * *'  # 6:30 PM EST
    - cron: '30 0 * * *'   # 8:30 PM EST
  workflow_dispatch:
    
permissions:
  contents: write
  actions: write

concurrency:
  group: cancel-stealth          # <-- Scrape duplicates
  cancel-in-progress: true           #      are cancelled

jobs:
  scrape_and_filter:
    runs-on: ubuntu-latest

    concurrency:
      group: dice-job-lock
      cancel-in-progress: false

    steps:
      # 0. Checkout your private repo containing the scraper code
      - name: Checkout private scripts
        uses: actions/checkout@v3
        with:
          repository: your-org/my-dice-jobs-scripts
          token: ${{ secrets.PRIVATE_SCRIPTS_PAT }}
          path: scripts

      # 1. Checkout the public repo (to commit outputs later)
      - name: Checkout public repo
        uses: actions/checkout@v3
        with:
          persist-credentials: false
          fetch-depth: 0

      # 2. Setup Python & Chrome
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install Chrome & Chromedriver
        run: |
          sudo apt-get update
          sudo apt-get install -y wget unzip curl
          wget https://storage.googleapis.com/chrome-for-testing-public/136.0.7103.94/linux64/chrome-linux64.zip
          unzip -q chrome-linux64.zip
          sudo mv chrome-linux64 /opt/chrome
          sudo ln -sf /opt/chrome/chrome /usr/bin/google-chrome
          wget https://storage.googleapis.com/chrome-for-testing-public/136.0.7103.94/linux64/chromedriver-linux64.zip
          unzip -q chromedriver-linux64.zip
          sudo mv chromedriver-linux64/chromedriver /usr/local/bin/
          sudo chmod +x /usr/local/bin/chromedriver
          google-chrome --version
          chromedriver --version

      # 3. Install dependencies from your private scripts repo
      - name: Install Python dependencies
        working-directory: scripts
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # 4. Run the scraper
      - name: Run scraper from private repo
        working-directory: scripts
        env:
          BASE_URL: ${{ secrets.BASE_URL }}
        run: python main.py

      # 5. Commit updated job files & logs back to the public repo
      - name: Commit and push updates
        run: |
          git config user.name  "github-actions"
          git config user.email "github-actions@github.com"
          git add output/jobs.csv output/final_ml_jobs.csv output/reposted_jobs.csv output/logs/*.log

          if git diff --cached --quiet; then
            echo "âœ… No changes to commit."
          else
            git commit -m "ðŸ”„ Auto-update jobs.csv and logs"
            git push https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git HEAD:${{ github.ref }}
          fi

      # 6. Trigger the apply workflow
      - name: Dispatch Apply workflow
        uses: benc-uk/workflow-dispatch@v1
        with:
          workflow: apply-and-commit.yml
          token: ${{ secrets.GITHUB_TOKEN }}
